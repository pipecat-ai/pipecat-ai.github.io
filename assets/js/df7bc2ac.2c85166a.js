"use strict";(self.webpackChunkpipecat_docs=self.webpackChunkpipecat_docs||[]).push([[169],{8186:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var s=t(4848),i=t(8453);const r={sidebar_position:4},a="Building Your Own Services",o={id:"understanding-bots/custom-services",title:"Building Your Own Services",description:"Once you've built a few simple bots by combining existing services, you'll want to start solving more complex problems, which means building your own services. Fortunately, it's pretty straightforward",source:"@site/docs/understanding-bots/custom-services.md",sourceDirName:"understanding-bots",slug:"/understanding-bots/custom-services",permalink:"/docs/understanding-bots/custom-services",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/understanding-bots/custom-services.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"The Interruptible Version",permalink:"/docs/understanding-bots/interruptible"},next:{title:"Debugging Pipecat apps",permalink:"/docs/understanding-bots/debugging"}},c={},l=[];function d(e){const n={a:"a",code:"code",h1:"h1",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"building-your-own-services",children:"Building Your Own Services"}),"\n",(0,s.jsxs)(n.p,{children:["Once you've built a few simple bots by combining existing services, you'll want to start solving more complex problems, which means building your own services. Fortunately, it's pretty straightforward: You create a subclass of ",(0,s.jsx)(n.code,{children:"AIService"}),", and implement a method called ",(0,s.jsx)(n.code,{children:"process_frame"}),". For example, here's a ",(0,s.jsx)(n.code,{children:"TranslationProcessor"})," service, used in the ",(0,s.jsx)(n.a,{href:"https://github.com/pipecat-ai/pipecat/tree/main/examples/tree/main/translation-chatbot",children:"translation example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TranslationProcessor(AIService):\n    def __init__(self, language):\n        self._language = language\n\n    async def process_frame(self, frame: Frame) -> AsyncGenerator[Frame, None]:\n        if isinstance(frame, TextFrame):\n            context = [\n                {\n                    "role": "system",\n                    "content": f"You will be provided with a sentence in English, and your task is to translate it into {self._language}.",\n                },\n                {"role": "user", "content": frame.text},\n            ]\n            yield LLMMessagesFrame(context)\n        else:\n            yield frame\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"__init__"})," method allows us to specify what language we want to use when we create an instance of the service."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"process_frame"})," method gets called by the pipeline with each frame emitted from the previous service (or the pipeline's input queue if this is the first service in the pipeline). In this case, if the current frame is a ",(0,s.jsx)(n.code,{children:"TextFrame"}),", we're putting the frame's text inside a ",(0,s.jsx)(n.code,{children:"context"})," with instructions for an LLM, and then putting that context inside an ",(0,s.jsx)(n.code,{children:"LLMMessagesFrame"})," that gets sent to the next service in the pipeline. If the current frame is anything other than a ",(0,s.jsx)(n.code,{children:"TextFrame"}),", we pass it along unmodified."]}),"\n",(0,s.jsxs)(n.p,{children:["This is an important convention you'll see in almost all services. ",(0,s.jsx)(n.strong,{children:"If your service receives a frame it doesn't do anything with, you should pass it along unmodified."})]}),"\n",(0,s.jsxs)(n.p,{children:["Another important thing to notice is the use of ",(0,s.jsx)(n.code,{children:"yield"})," throughout the method, as well as the return type of the ",(0,s.jsx)(n.code,{children:"process_frame"})," function: ",(0,s.jsx)(n.code,{children:"AsyncGenerator[Frame, None]"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["If you're just getting into Python because of all the interesting things happening with AI, you should familiarize yourself with ",(0,s.jsx)(n.a,{href:"https://wiki.python.org/moin/Generators",children:"Python's Generators"}),", and more specifically, ",(0,s.jsx)(n.a,{href:"https://superfastpython.com/asynchronous-generators-in-python/",children:"async generators and asyncio"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The pipeline actually calls your ",(0,s.jsx)(n.code,{children:"process_frame"})," function with each input frame like this: ",(0,s.jsx)(n.code,{children:"async for frame in service.process_frame(frame):"}),", which means you can take advantage of multiple ",(0,s.jsx)(n.code,{children:"yield"}),' statements to create multiple output frames from a single input frame. For example, here\'s a custom service that enables extremely basic animation by displaying a "talking" image while the bot is talking:']}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ImageSyncAggregator(AIService):\n    async def process_frame(self, frame: Frame) -> AsyncGenerator[Frame, None]:\n        if isinstance(frame, TextFrame):\n            yield talking_frame\n            yield frame\n            yield quiet_frame\n        else:\n            yield frame\n"})}),"\n",(0,s.jsxs)(n.p,{children:["When this service receives a ",(0,s.jsx)(n.code,{children:"TextFrame"}),", it yields an ",(0,s.jsx)(n.code,{children:"ImageFrame"})," that contains a character with its mouth open, then it yields the received ",(0,s.jsx)(n.code,{children:"TextFrame"}),", then it yields an ",(0,s.jsx)(n.code,{children:"ImageFrame"}),' with the character\'s mouth closed. Because of the way subsequent services (like TTS) keep frames in order, the end result will be the transport displaying the "talking" image, then playing back the TTS audio, then displaying the "quiet" image. As before, anything that isn\'t a ',(0,s.jsx)(n.code,{children:"TextFrame"})," gets passed along unmodified."]}),"\n",(0,s.jsx)(n.p,{children:"Eventually, you'll find your bot doing something you don't expect. Let's learn about how to debug Pipecat apps in the next section."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);