"use strict";(self.webpackChunkpipecat_docs=self.webpackChunkpipecat_docs||[]).push([[289],{535:(e,s,r)=>{r.r(s),r.d(s,{assets:()=>c,contentTitle:()=>i,default:()=>g,frontMatter:()=>n,metadata:()=>o,toc:()=>l});var a=r(4848),t=r(8453);const n={},i="Utility Services",o={id:"api-reference/services/utilities",title:"Utility Services",description:"In addition to the various AI services available in Pipecat, there are a handful of utility service classes that help you build your app.",source:"@site/docs/api-reference/services/utilities.md",sourceDirName:"api-reference/services",slug:"/api-reference/services/utilities",permalink:"/docs/api-reference/services/utilities",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/api-reference/services/utilities.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TTSService (Text-To-Speech)",permalink:"/docs/api-reference/services/tts-service"},next:{title:"VisionService (Image Processing and Recognition)",permalink:"/docs/api-reference/services/vision-service"}},c={},l=[{value:"Aggregators",id:"aggregators",level:2},{value:"<code>LLMUserResponseAggregator</code> and <code>LLMAssistantResponseAggregator</code>",id:"llmuserresponseaggregator-and-llmassistantresponseaggregator",level:3},{value:"<code>UserResponseAggregator</code>",id:"userresponseaggregator",level:3},{value:"<code>LLMFullResponseAggregator</code>",id:"llmfullresponseaggregator",level:3},{value:"<code>SentenceAggregator</code>",id:"sentenceaggregator",level:3},{value:"<code>StatelessTextTransformer</code>",id:"statelesstexttransformer",level:3},{value:"Advanced Aggregators",id:"advanced-aggregators",level:2},{value:"<code>ParallelPipeline</code>",id:"parallelpipeline",level:3},{value:"<code>GatedAggregator</code>",id:"gatedaggregator",level:3},{value:"<code>VisionImageFrameAggregator</code>",id:"visionimageframeaggregator",level:3}];function d(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.h1,{id:"utility-services",children:"Utility Services"}),"\n",(0,a.jsx)(s.p,{children:"In addition to the various AI services available in Pipecat, there are a handful of utility service classes that help you build your app."}),"\n",(0,a.jsx)(s.h2,{id:"aggregators",children:"Aggregators"}),"\n",(0,a.jsxs)(s.p,{children:["These should all be imported from ",(0,a.jsx)(s.code,{children:"pipecat.pipeline.aggregators"}),"."]}),"\n",(0,a.jsxs)(s.h3,{id:"llmuserresponseaggregator-and-llmassistantresponseaggregator",children:[(0,a.jsx)(s.code,{children:"LLMUserResponseAggregator"})," and ",(0,a.jsx)(s.code,{children:"LLMAssistantResponseAggregator"})]}),"\n",(0,a.jsxs)(s.p,{children:["These services are the best way to take a conversation between a user and a bot and save it to a ",(0,a.jsx)(s.a,{href:"https://platform.openai.com/docs/guides/text-generation/chat-completions-api",children:"list of messages"})," that can be used for LLM completions. Both service initializers take a ",(0,a.jsx)(s.code,{children:"messages"})," object, which you should create in your app and pass to both services, like this:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"messages = []\nuser_aggregator = LLMUserResponseAggregator(messages, transport._my_participant_id)\nbot_aggregator = LLMAssistantResponseAggregator(messages, transport._my_participant_id)\n"})}),"\n",(0,a.jsx)(s.p,{children:"Each service listens for a start frame, one or more text frames, and an end frame."}),"\n",(0,a.jsx)(s.p,{children:"LLMUserResponseAggregator:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["Start: ",(0,a.jsx)(s.code,{children:"UserStartedSpeakingFrame"})]}),"\n",(0,a.jsxs)(s.li,{children:["Text: ",(0,a.jsx)(s.code,{children:"TranscriptionFrame"})]}),"\n",(0,a.jsxs)(s.li,{children:["End: ",(0,a.jsx)(s.code,{children:"UserStoppedSpeakingFrame"})]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"(Note: This uses VAD to generate the started and stopped speaking frames.)"}),"\n",(0,a.jsx)(s.p,{children:"LLMAssistantResponseAggregator:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["Start: ",(0,a.jsx)(s.code,{children:"LLMResponseStartFrame"})]}),"\n",(0,a.jsx)(s.li,{children:"Text: `TextFrame"}),"\n",(0,a.jsxs)(s.li,{children:["End: ",(0,a.jsx)(s.code,{children:"LLMResponseEndFrame"})]}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["You can see this in action in ",(0,a.jsx)(s.a,{href:"https://github.com/pipecat-ai/pipecat/tree/main/examples/blob/b8823527065d4086f9a98f8008cc51d64f3ce969/chatbot/bot.py",children:"the chatbot example app"}),"."]}),"\n",(0,a.jsx)(s.h3,{id:"userresponseaggregator",children:(0,a.jsx)(s.code,{children:"UserResponseAggregator"})}),"\n",(0,a.jsxs)(s.p,{children:["If you want to accumulate user speech between 'started' and 'stopped' talking events, but you don't want to store that in a ",(0,a.jsx)(s.code,{children:"messages"})," list for an LLM, you can use a ",(0,a.jsx)(s.code,{children:"UserResponseAggregator"}),". This will accumulate all ",(0,a.jsx)(s.code,{children:"TranscriptionFrame"}),"s received between a ",(0,a.jsx)(s.code,{children:"UserStartedSpeakingFrame"})," and a ",(0,a.jsx)(s.code,{children:"UserStoppedSpeakingFrame"}),", then emit them as a single ",(0,a.jsx)(s.code,{children:"TextFrame"}),"."]}),"\n",(0,a.jsx)(s.h3,{id:"llmfullresponseaggregator",children:(0,a.jsx)(s.code,{children:"LLMFullResponseAggregator"})}),"\n",(0,a.jsxs)(s.p,{children:["The same as the ",(0,a.jsx)(s.code,{children:"UserResponseAggregator"}),", but, you know, for the LLM."]}),"\n",(0,a.jsx)(s.h3,{id:"sentenceaggregator",children:(0,a.jsx)(s.code,{children:"SentenceAggregator"})}),"\n",(0,a.jsxs)(s.p,{children:["This uses an approach similar to the approach used by text-to-speech services: It will accumulate ",(0,a.jsx)(s.code,{children:"TextFrame"}),"s until it sees one with sentence-ending punctuation, then emit all of the accumulated text as one frame. So, for example:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{children:'<TextFrame text="Hello, it\'s">\n<TextFrame text=" nice to meet">\n<TextFrame text=" you.">\n'})}),"\n",(0,a.jsxs)(s.p,{children:["Becomes a single TextFrame containing the text ",(0,a.jsx)(s.code,{children:'"Hello, it\'s nice to meet you."'})]}),"\n",(0,a.jsx)(s.h3,{id:"statelesstexttransformer",children:(0,a.jsx)(s.code,{children:"StatelessTextTransformer"})}),"\n",(0,a.jsxs)(s.p,{children:["This service transforms the text of any ",(0,a.jsx)(s.code,{children:"TextFrame"}),"s it sees. For example, this instance:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"aggregator = StatelessTextTransformer(lambda x: x.upper())\n"})}),"\n",(0,a.jsxs)(s.p,{children:["Would receive ",(0,a.jsx)(s.code,{children:'<TextFrame text="Hello!">'})," and emit ",(0,a.jsx)(s.code,{children:'<TextFrame text="HELLO!">'}),"."]}),"\n",(0,a.jsx)(s.h2,{id:"advanced-aggregators",children:"Advanced Aggregators"}),"\n",(0,a.jsx)(s.p,{children:"These services enable some complex pipeline architectures."}),"\n",(0,a.jsx)(s.h3,{id:"parallelpipeline",children:(0,a.jsx)(s.code,{children:"ParallelPipeline"})}),"\n",(0,a.jsxs)(s.p,{children:["This service lets you run a set of services in parallel, instead of sequentially. For example, it's used in ",(0,a.jsx)(s.code,{children:"05-sync-speech-and-image"})," in ",(0,a.jsx)(s.a,{href:"https://github.com/daily-co/daily-ai-sdk/blob/27322108b728adda3708fb59e2ff20e3183efc4e/examples/foundational/05-sync-speech-and-image.py",children:"the framework repo"}),":"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"pipeline = Pipeline(\n    processors=[\n        llm,\n        sentence_aggregator,\n        ParallelPipeline(\n            [[month_prepender, tts], [llm_full_response_aggregator, imagegen]]\n        ),\n        gated_aggregator,\n    ],\n)\n"})}),"\n",(0,a.jsx)(s.p,{children:"It's important to keep in mind that the parallel pipeline doesn't let frames get 'out of order'; it takes frame A from its source queue and sends it through all of the parallel pipelines at the same time, but it doesn't start processing frame B from its source queue until all of the parallel branches from frame A have completed and yielded frames to the sink queue."}),"\n",(0,a.jsx)(s.h3,{id:"gatedaggregator",children:(0,a.jsx)(s.code,{children:"GatedAggregator"})}),"\n",(0,a.jsxs)(s.p,{children:["For more information on the ",(0,a.jsx)(s.code,{children:"GatedAggregator"}),", take a look at ",(0,a.jsx)(s.a,{href:"https://github.com/daily-co/daily-ai-sdk/blob/e22babbae2ef33454158b59831114734adf5f5d8/examples/foundational/05-sync-speech-and-image.py",children:"this example in the framework"}),", as well as ",(0,a.jsx)(s.a,{href:"https://github.com/pipecat-ai/pipecat/blob/db05a9b29b24d483815b60a3e727fae3f874666d/src/pipecat/pipeline/aggregators.py#L418",children:"the comments in the source code"}),"."]}),"\n",(0,a.jsx)(s.h3,{id:"visionimageframeaggregator",children:(0,a.jsx)(s.code,{children:"VisionImageFrameAggregator"})}),"\n",(0,a.jsxs)(s.p,{children:["Use a ",(0,a.jsx)(s.code,{children:"VisionImageFrameAggregator"})," to build a ",(0,a.jsx)(s.code,{children:"VisionImageFrame"})," out of a ",(0,a.jsx)(s.code,{children:"TextFrame"})," and an ",(0,a.jsx)(s.code,{children:"ImageFrame"}),". See ",(0,a.jsx)(s.a,{href:"vision-service",children:"Vision Services"})," for more info."]})]})}function g(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,s,r)=>{r.d(s,{R:()=>i,x:()=>o});var a=r(6540);const t={},n=a.createContext(t);function i(e){const s=a.useContext(n);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(n.Provider,{value:s},e.children)}}}]);