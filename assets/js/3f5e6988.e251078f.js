"use strict";(self.webpackChunkpipecat_docs=self.webpackChunkpipecat_docs||[]).push([[829],{9567:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var s=n(4848),i=n(8453);const r={sidebar_position:2},o="From Theory to Practice: A Simplified Bot",a={id:"understanding-bots/from-theory-to-practice",title:"From Theory to Practice: A Simplified Bot",description:"Let's create a pipeline for a basic chatbot to see how it all works. Here's a lightly modified version of examples/foundational/06-listen-and-respond.py:",source:"@site/docs/understanding-bots/from-theory-to-practice.md",sourceDirName:"understanding-bots",slug:"/understanding-bots/from-theory-to-practice",permalink:"/docs/understanding-bots/from-theory-to-practice",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/understanding-bots/from-theory-to-practice.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"The Architecture of Pipecat",permalink:"/docs/understanding-bots/dailyai-architecture"},next:{title:"The Interruptible Version",permalink:"/docs/understanding-bots/interruptible"}},c={},d=[];function l(e){const t={code:"code",h1:"h1",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"from-theory-to-practice-a-simplified-bot",children:"From Theory to Practice: A Simplified Bot"}),"\n",(0,s.jsxs)(t.p,{children:["Let's create a pipeline for a basic chatbot to see how it all works. Here's a lightly modified version of ",(0,s.jsx)(t.code,{children:"examples/foundational/06-listen-and-respond.py"}),":"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'async def main(room_url: str, token):\n    async with aiohttp.ClientSession() as session:\n        messages = [\n            {\n                "role": "system",\n                "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio. Respond to what the user said in a creative and helpful way.",\n            },\n        ]\n\n\t\ttransport = DailyTransport(...)\n\n        tts = ElevenLabsTTSService(...)\n        llm = OpenAILLMService(...)\n\n        tma_in = LLMUserContextAggregator(messages, ...)\n        tma_out = LLMAssistantContextAggregator(messages, ...)\n\n        pipeline = Pipeline(\n            processors=[\n                tma_in,\n                llm,\n                tts,\n                tma_out,\n            ],\n        )\n\n        @transport.event_handler("on_first_other_participant_joined")\n        async def on_first_other_participant_joined(transport):\n            # Kick off the conversation.\n            messages.append(\n                {"role": "system", "content": "Please introduce yourself to the user."})\n            await pipeline.queue_frames([LLMMessagesFrame(messages)])\n\n\t\t(...)\n        await transport.run(pipeline)\n'})}),"\n",(0,s.jsxs)(t.p,{children:["We start by creating a ",(0,s.jsx)(t.code,{children:"messages"})," object, which will be shared by a few different services."]}),"\n",(0,s.jsxs)(t.p,{children:["Next, we create a ",(0,s.jsx)(t.code,{children:"DailyTransport"})," object. If you go to the example file, you can see the full list of all the things we're configuring about the transport. The ",(0,s.jsx)(t.code,{children:"DailyTransport"})," uses Daily's WebRTC-powered infrastructure as the media transport layer."]}),"\n",(0,s.jsxs)(t.p,{children:["Then, we instantiate some services that we'll use in our pipeline. There are a bunch of different services you can use, and services of the same type are usually interchangeable. For example, we could easily swap out ",(0,s.jsx)(t.code,{children:"ElevenLabsTTSService"})," for ",(0,s.jsx)(t.code,{children:"DeepgramTTSService"})," if we wanted to use Deepgram's Aura voice engine instead. Or we could use Mistral running on Groq by changing out ",(0,s.jsx)(t.code,{children:"OpenAILLMService"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["Next are two other important services provided by the framework itself. ",(0,s.jsx)(t.code,{children:"LLMUserContextAggregator"})," grabs user input (in the form of ",(0,s.jsx)(t.code,{children:"TranscriptionFrame"}),"s), appends it to the shared ",(0,s.jsx)(t.code,{children:"messages"})," object as a ",(0,s.jsx)(t.code,{children:"user"})," entry, and then emits ",(0,s.jsx)(t.code,{children:"LLMMessageFrame"}),"s for the LLM service to use. Likewise, ",(0,s.jsx)(t.code,{children:"LLMAssistantContextAggregator"})," captures the ",(0,s.jsx)(t.code,{children:"TextFrame"}),"s created by the LLM service to add an ",(0,s.jsx)(t.code,{children:"assistant"})," entry to the ",(0,s.jsx)(t.code,{children:"messages"})," object."]}),"\n",(0,s.jsxs)(t.p,{children:["Then we define the ",(0,s.jsx)(t.code,{children:"pipeline"})," that will run in this app. It's composed of:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"tma_in"}),", which receives ",(0,s.jsx)(t.code,{children:"TranscriptionFrame"}),"s from the transport when the user speaks, and emits ",(0,s.jsx)(t.code,{children:"LLMMessagesFrame"}),"s"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"llm"}),", which receives ",(0,s.jsx)(t.code,{children:"LLMMessagesFrame"}),"s, runs a chat completion on OpenAI, and emits the streaming result as a series of ",(0,s.jsx)(t.code,{children:"TextFrame"}),"s"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"tts"}),", which receives the ",(0,s.jsx)(t.code,{children:"TextFrame"}),"s and accumulates them until it has at least a complete sentence, then runs text-to-speech and emits ",(0,s.jsx)(t.code,{children:"AudioFrame"}),"s with the spoken audio. ",(0,s.jsx)(t.code,{children:"tts"})," also emits a ",(0,s.jsx)(t.code,{children:"TextFrame"})," with the text of any audio it speaks."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"tma_out"}),", which receives the ",(0,s.jsx)(t.code,{children:"TextFrame"}),"s from the ",(0,s.jsx)(t.code,{children:"tts"})," service and appends them to the ",(0,s.jsx)(t.code,{children:"messages"})," object. ",(0,s.jsx)(t.code,{children:"tma_out"})," also passes along the ",(0,s.jsx)(t.code,{children:"AudioFrame"}),"s from ",(0,s.jsx)(t.code,{children:"tts"})," so they exit the pipeline and get sent to the transport, which then plays back the audio in the Daily call."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["We'll come back to ",(0,s.jsx)(t.code,{children:"on_first_other_participant_joined"}),". The last line, ",(0,s.jsx)(t.code,{children:"await transport.run(pipeline)"}),", is what makes the whole thing work."]}),"\n",(0,s.jsxs)(t.p,{children:["Under the hood, a ",(0,s.jsx)(t.code,{children:"pipeline"})," object has a ",(0,s.jsx)(t.code,{children:"run(input_queue, output_queue)"})," method. The pipeline pulls frames out of the ",(0,s.jsx)(t.code,{children:"input_queue"})," and sends them to the first service in the pipeline. Whatever comes out of the last service in the pipeline gets put into the ",(0,s.jsx)(t.code,{children:"output_queue"}),". ",(0,s.jsx)(t.code,{children:"transport.run(pipeline)"})," is just shorthand for ",(0,s.jsx)(t.code,{children:"pipeline.run(input_queue=transport.output, output_queue=transport.input"}),", along with code for running the transport itself. ",(0,s.jsx)(t.code,{children:"pipeline.run"})," watches for an ",(0,s.jsx)(t.code,{children:"EndFrame"})," from the transport to know when to exit."]}),"\n",(0,s.jsxs)(t.p,{children:["Once everything is running, ",(0,s.jsx)(t.code,{children:"on_first_other_participant_joined()"})," is what actually starts the conversation. The transport provides a decorator, ",(0,s.jsx)(t.code,{children:'@transport.event_handler("on_first_other_participant_joined")'}),", that does what it says: Runs a function when a participant other than the bot itself joins the Daily room. In this case, we're constructing our own ",(0,s.jsx)(t.code,{children:"LLMMessagesFrame"})," and passing it directly into the pipeline by calling ",(0,s.jsx)(t.code,{children:"pipeline.queue_frames()"}),". The first service in the pipeline (",(0,s.jsx)(t.code,{children:"tma_in"}),") doesn't know what to do with an ",(0,s.jsx)(t.code,{children:"LLMMessagesFrame"})," , so it passes it along to the ",(0,s.jsx)(t.code,{children:"llm"})," service... which will generate a friendly greeting from the bot."]}),"\n",(0,s.jsx)(t.p,{children:"Next, we'll see how to implement one of the most important things for making a bot feel interactive: Interruption."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var s=n(6540);const i={},r=s.createContext(i);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);