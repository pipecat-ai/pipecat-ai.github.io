"use strict";(self.webpackChunkpipecat_docs=self.webpackChunkpipecat_docs||[]).push([[7274],{1079:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>c,toc:()=>a});var i=s(4848),r=s(8453);const t={},o="LLMService (LLM Completion)",c={id:"api-reference/services/llm-service",title:"LLMService (LLM Completion)",description:"Frame Types",source:"@site/docs/api-reference/services/llm-service.md",sourceDirName:"api-reference/services",slug:"/api-reference/services/llm-service",permalink:"/docs/api-reference/services/llm-service",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/api-reference/services/llm-service.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ImageGenService (Image Generation)",permalink:"/docs/api-reference/services/image-gen-service"},next:{title:"STTService (Speech-To-Text)",permalink:"/docs/api-reference/services/stt-service"}},l={},a=[{value:"Frame Types",id:"frame-types",level:2},{value:"Configuration",id:"configuration",level:2},{value:"LLM Context",id:"llm-context",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"llmservice-llm-completion",children:"LLMService (LLM Completion)"}),"\n",(0,i.jsx)(n.h2,{id:"frame-types",children:"Frame Types"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," ",(0,i.jsx)(n.code,{children:"LLMMessagesFrame"}),", or ",(0,i.jsx)(n.code,{children:"OpenAILLMContextFrame"})," for OpenAI-compatible LLM services"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output:"})," LLM services yield these frames in this order for chat completions:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"LLMResponseStartFrame"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"TextFrame"}),"s. Some services (like GPT-4) stream responses one word (token) at a time. Others, like Groq, will include the entire response in a single TextFrame."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"LLMResponseEndFrame"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"If the LLM is completing a function call instead of a chat response, it will yield these frames:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"LLMResponseStartFrame"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"LLMFunctionStartFrame"})," with the name of the function being called"]}),"\n",(0,i.jsxs)(n.li,{children:["A single ",(0,i.jsx)(n.code,{children:"LLMFunctionCallFrame"})," with the function name and arguments. Even if the service itself streams responses, the LLMService will aggregate them all together and yield a single ",(0,i.jsx)(n.code,{children:"LLMFunctionCallFrame"}),"."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"LLMResponseEndFrame"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Each LLM service initializer takes a slightly different parameter set. Some allow you to specify a model. Most require an API key."}),"\n",(0,i.jsxs)(n.p,{children:["Some services, such as OpenAI, support ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"function calling"}),". Those service will accept ",(0,i.jsx)(n.code,{children:"tools"})," and ",(0,i.jsx)(n.code,{children:"tool_choice"})," in their initializers. Take a look at the ",(0,i.jsx)(n.code,{children:"patient-intake"})," example for a good look at function calling."]}),"\n",(0,i.jsx)(n.h2,{id:"llm-context",children:"LLM Context"}),"\n",(0,i.jsxs)(n.p,{children:["LLM services do not keep their own history. It's up to you to build your own ",(0,i.jsx)(n.code,{children:"messages"})," object to pass into the LLM service as a ",(0,i.jsx)(n.code,{children:"OpenAILLMContextFrame"})," or ",(0,i.jsx)(n.code,{children:"LLMMessagesFrame"}),". Fortunately, there are other tools in the framework to help with this. Take a look at ",(0,i.jsx)(n.a,{href:"utilities#aggregators",children:"the Aggregators section of the Utilities docs"})," to learn more."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>c});var i=s(6540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);